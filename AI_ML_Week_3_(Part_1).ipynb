{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI/ML Week 3 (Part 1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcvVPP85UCZQ/Cq0fhctYr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eStU06mVxqlU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "f244598c-6582-4112-f391-1f2f750a831a"
      },
      "source": [
        "!wget https://github.com/DeepConnectAI/challenge-week-3/raw/master/data/divorce.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-26 18:58:01--  https://github.com/DeepConnectAI/challenge-week-3/raw/master/data/divorce.csv\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeepConnectAI/challenge-week-3/master/data/divorce.csv [following]\n",
            "--2020-08-26 18:58:01--  https://raw.githubusercontent.com/DeepConnectAI/challenge-week-3/master/data/divorce.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19021 (19K) [text/plain]\n",
            "Saving to: ‘divorce.csv.1’\n",
            "\n",
            "divorce.csv.1       100%[===================>]  18.58K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-08-26 18:58:01 (2.67 MB/s) - ‘divorce.csv.1’ saved [19021/19021]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC_QKJHj7MoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oLwB09_ArJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "10d24b11-d471-4ad7-9c4f-0879fb78f762"
      },
      "source": [
        "#Read data\n",
        "data = pd.read_csv(\"/content/divorce.csv\", delimiter=';')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Atr1</th>\n",
              "      <th>Atr2</th>\n",
              "      <th>Atr3</th>\n",
              "      <th>Atr4</th>\n",
              "      <th>Atr5</th>\n",
              "      <th>Atr6</th>\n",
              "      <th>Atr7</th>\n",
              "      <th>Atr8</th>\n",
              "      <th>Atr9</th>\n",
              "      <th>Atr10</th>\n",
              "      <th>Atr11</th>\n",
              "      <th>Atr12</th>\n",
              "      <th>Atr13</th>\n",
              "      <th>Atr14</th>\n",
              "      <th>Atr15</th>\n",
              "      <th>Atr16</th>\n",
              "      <th>Atr17</th>\n",
              "      <th>Atr18</th>\n",
              "      <th>Atr19</th>\n",
              "      <th>Atr20</th>\n",
              "      <th>Atr21</th>\n",
              "      <th>Atr22</th>\n",
              "      <th>Atr23</th>\n",
              "      <th>Atr24</th>\n",
              "      <th>Atr25</th>\n",
              "      <th>Atr26</th>\n",
              "      <th>Atr27</th>\n",
              "      <th>Atr28</th>\n",
              "      <th>Atr29</th>\n",
              "      <th>Atr30</th>\n",
              "      <th>Atr31</th>\n",
              "      <th>Atr32</th>\n",
              "      <th>Atr33</th>\n",
              "      <th>Atr34</th>\n",
              "      <th>Atr35</th>\n",
              "      <th>Atr36</th>\n",
              "      <th>Atr37</th>\n",
              "      <th>Atr38</th>\n",
              "      <th>Atr39</th>\n",
              "      <th>Atr40</th>\n",
              "      <th>Atr41</th>\n",
              "      <th>Atr42</th>\n",
              "      <th>Atr43</th>\n",
              "      <th>Atr44</th>\n",
              "      <th>Atr45</th>\n",
              "      <th>Atr46</th>\n",
              "      <th>Atr47</th>\n",
              "      <th>Atr48</th>\n",
              "      <th>Atr49</th>\n",
              "      <th>Atr50</th>\n",
              "      <th>Atr51</th>\n",
              "      <th>Atr52</th>\n",
              "      <th>Atr53</th>\n",
              "      <th>Atr54</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Atr1  Atr2  Atr3  Atr4  Atr5  Atr6  ...  Atr50  Atr51  Atr52  Atr53  Atr54  Class\n",
              "0     2     2     4     1     0     0  ...      3      2      3      2      1      1\n",
              "1     4     4     4     4     4     0  ...      4      4      4      2      2      1\n",
              "2     2     2     2     2     1     3  ...      1      1      2      2      2      1\n",
              "3     3     2     3     2     3     3  ...      3      3      2      2      2      1\n",
              "4     2     2     1     1     1     1  ...      2      2      2      1      0      1\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-rb8vu-A1oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Insert bias column\n",
        "data.insert(0,'Bias', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDkt8zKLBeaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "08e69bd8-7b5a-475c-85aa-171a010054c4"
      },
      "source": [
        "#Define X and Y\n",
        "X = data.drop(['Class'],1)\n",
        "y = data['Class']\n",
        "X,y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     Bias  Atr1  Atr2  Atr3  Atr4  ...  Atr50  Atr51  Atr52  Atr53  Atr54\n",
              " 0       1     2     2     4     1  ...      3      2      3      2      1\n",
              " 1       1     4     4     4     4  ...      4      4      4      2      2\n",
              " 2       1     2     2     2     2  ...      1      1      2      2      2\n",
              " 3       1     3     2     3     2  ...      3      3      2      2      2\n",
              " 4       1     2     2     1     1  ...      2      2      2      1      0\n",
              " ..    ...   ...   ...   ...   ...  ...    ...    ...    ...    ...    ...\n",
              " 165     1     0     0     0     0  ...      1      4      2      2      2\n",
              " 166     1     0     0     0     0  ...      2      2      3      2      2\n",
              " 167     1     1     1     0     0  ...      1      1      3      0      0\n",
              " 168     1     0     0     0     0  ...      3      2      4      3      1\n",
              " 169     1     0     0     0     0  ...      1      3      3      3      1\n",
              " \n",
              " [170 rows x 55 columns], 0      1\n",
              " 1      1\n",
              " 2      1\n",
              " 3      1\n",
              " 4      1\n",
              "       ..\n",
              " 165    0\n",
              " 166    0\n",
              " 167    0\n",
              " 168    0\n",
              " 169    0\n",
              " Name: Class, Length: 170, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXejT-TNB86g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c34e86d4-ee87-4171-f1f0-40c8193b5165"
      },
      "source": [
        "X_shape = X.shape\n",
        "X_type  = type(X)\n",
        "y_shape = y.shape\n",
        "y_type  = type(y)\n",
        "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
        "print(f'y: Type-{y_type}, Shape-{y_shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: Type-<class 'pandas.core.frame.DataFrame'>, Shape-(170, 55)\n",
            "y: Type-<class 'pandas.core.series.Series'>, Shape-(170,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJZ6hY9VFTZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ze3rgxYLLaq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2ec22a29-6ee9-43b6-ac06-fac7e463a87f"
      },
      "source": [
        "X_train_shape = X_train.shape\n",
        "y_train_shape = y_train.shape\n",
        "X_test_shape  = X_test.shape\n",
        "y_test_shape  = y_test.shape\n",
        "\n",
        "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
        "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
        "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (144, 55) , y_train: (144,)\n",
            "X_test: (26, 55) , y_test: (26,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2fonQmnl1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
        "        '''Initialize variables\n",
        "        Args:\n",
        "            learning_rate  : Learning Rate\n",
        "            max_iterations : Max iterations for training weights\n",
        "        '''\n",
        "        # Initialising all the parameters\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.likelihoods    = []\n",
        "        \n",
        "        # Define epsilon because log(0) is not defined\n",
        "        self.eps = 1e-7\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''Sigmoid function: f:R->(0,1)\n",
        "        Args:\n",
        "            z : A numpy array (num_samples,)\n",
        "        Returns:\n",
        "            A numpy array where sigmoid function applied to every element\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        sig_z = 1/(1+np.exp(-z))\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        assert z.shape==sig_z.shape, 'Error in sigmoid implementation. Check carefully'\n",
        "        return sig_z\n",
        "    \n",
        "    def log_likelihood(self, y_true, y_pred):\n",
        "        '''Calculates maximum likelihood estimate\n",
        "        Remember: y * log(yh) + (1-y) * log(1-yh)\n",
        "        Note: Likelihood is defined for multiple classes as well, but for this dataset\n",
        "        we only need to worry about binary/bernoulli likelihood function\n",
        "        Args:\n",
        "            y_true : Numpy array of actual truth values (num_samples,)\n",
        "            y_pred : Numpy array of predicted values (num_samples,)\n",
        "        Returns:\n",
        "            Log-likelihood, scalar value\n",
        "        '''\n",
        "        # Fix 0/1 values in y_pred so that log is not undefined\n",
        "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        likelihood = np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return likelihood\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        '''Trains logistic regression model using gradient ascent\n",
        "        to gain maximum likelihood on the training data\n",
        "        Args:\n",
        "            X : Numpy array (num_examples, num_features)\n",
        "            y : Numpy array (num_examples, )\n",
        "        Returns: VOID\n",
        "        '''\n",
        "        \n",
        "        num_examples = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        \n",
        "        # Initialize weights with appropriate shape\n",
        "        import random\n",
        "        self.weights = np.random.random(num_features,)\n",
        "        #print(self.weights)\n",
        "        # Perform gradient ascent\n",
        "        for i in range(self.max_iterations):\n",
        "            print(i, sep=\",\",end=',')\n",
        "            # Define the linear hypothesis(z) first\n",
        "            # HINT: what is our hypothesis function in linear regression, remember?\n",
        "            #print(weights)\n",
        "            z = X@self.weights\n",
        "\n",
        "            \n",
        "            # Output probability value by appplying sigmoid on z\n",
        "            y_pred = self.sigmoid(z)\n",
        "            \n",
        "            # Calculate the gradient values\n",
        "            # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.\n",
        "            gradient = np.mean((y-y_pred)*X.T, axis=1)\n",
        "            \n",
        "            # Update the weights\n",
        "            # Caution: It is gradient ASCENT not descent\n",
        "            self.weights = self.weights + self.learning_rate*gradient\n",
        "            \n",
        "            # Calculating log likelihood\n",
        "            #likelihood = self.log_likelihood(self, y, y_pred)\n",
        "            y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
        "        \n",
        "        ### START CODE HERE\n",
        "            likelihood = np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
        "\n",
        "            self.likelihoods.append(likelihood)\n",
        "    \n",
        "        ### END CODE HERE\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        '''Predict probabilities for given X.\n",
        "        Remember sigmoid returns value between 0 and 1.\n",
        "        Args:\n",
        "            X : Numpy array (num_samples, num_features)\n",
        "        Returns:\n",
        "            probabilities: Numpy array (num_samples,)\n",
        "        '''\n",
        "        if self.weights is None:\n",
        "            raise Exception(\"Fit the model before prediction\")\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        z = X@self.weights\n",
        "        probabilities = self.sigmoid(z)\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return probabilities\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        '''Predict/Classify X in classes\n",
        "        Args:\n",
        "            X         : Numpy array (num_samples, num_features)\n",
        "            threshold : scalar value above which prediction is 1 else 0\n",
        "        Returns:\n",
        "            binary_predictions : Numpy array (num_samples,)\n",
        "        '''\n",
        "        # Thresholding probability to predict binary values\n",
        "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
        "        \n",
        "        return binary_predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G7TRYEm6p0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now initialize logitic regression implemented by you\n",
        "model = MyLogisticRegression(0.01,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BWU0VjEr8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "50479a4e-5719-435d-e131-7c6bd165e835"
      },
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5px79HksOyVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b02a31fc-9558-467c-d726-b8f11ce99a80"
      },
      "source": [
        "# Train log-likelihood\n",
        "train_log_likelihood = model.log_likelihood(y_train, model.predict_proba(X_train))\n",
        "print(\"Log-likelihood on training data:\", train_log_likelihood)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log-likelihood on training data: -0.09545332025337111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTtcvbvNamQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aaa4209a-3b14-4d93-f1e0-7e947f73291f"
      },
      "source": [
        "# Test log-likelihood\n",
        "test_log_likelihood = model.log_likelihood(y_test, model.predict_proba(X_test))\n",
        "print(\"Log-likelihood on testing data:\", test_log_likelihood)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log-likelihood on testing data: -0.05210138716043384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KZ2X5uHame3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "8def1c65-779c-4b54-f641-4dbf28b3266c"
      },
      "source": [
        "# Plot the loss curve\n",
        "plt.plot([i+1 for i in range(len(model.likelihoods))], model.likelihoods)\n",
        "plt.title(\"Log-Likelihood curve\")\n",
        "plt.xlabel(\"Iteration num\")\n",
        "plt.ylabel(\"Log-likelihood\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNIfKyBeamM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make predictions on test data\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFn2p8yamHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_true,y_pred):\n",
        "    '''Compute accuracy.\n",
        "    Accuracy = (Correct prediction / number of samples)\n",
        "    Args:\n",
        "        y_true : Truth binary values (num_examples, )\n",
        "        y_pred : Predicted binary values (num_examples, )\n",
        "    Returns:\n",
        "        accuracy: scalar value\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE\n",
        "\n",
        "    tp=0\n",
        "    #tn=0\n",
        "    for i in range(len(y_pred)):\n",
        "      #print(y_true[i],\",\",y_pred[i])\n",
        "      if y_true[i] == y_pred[i]:\n",
        "        tp+=1\n",
        "    accuracy = tp/len(y_true)\n",
        "    ### END CODE HERE\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RgPafaha2V-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a01702e-0135-4cd3-f1fd-2c74dd7b2fd7"
      },
      "source": [
        "# Print accuracy on train data\n",
        "y_pred=model.predict(X_train)\n",
        "#np.array(y_train)\n",
        "accuracy(np.array(y_train), y_pred)\n",
        "#0.9915966386554622 accuracy with 30% test set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9791666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2vGSAr_a2_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5daf096-2737-44d0-dbbd-25492ac3bf35"
      },
      "source": [
        "# Print accuracy on test data\n",
        "y_pred=model.predict(X_test)\n",
        "#np.array(y_train)\n",
        "accuracy(np.array(y_test), y_pred)\n",
        "#0.9411764705882353 accuracy with 30% test set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9615384615384616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3MlMqSee2f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Logistic Regression using sklearn\n",
        "#Part 1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt79ogGLe2mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhuBCeQ2e7-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define X and y\n",
        "X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY5ZhaiMfNPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the model from sklearn\n",
        "model2 = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kgUpjMafXmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toJW2or_fdO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model2.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0FIT-LhfmsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "52bcebe9-78f6-4f38-96b8-f67c80a3098d"
      },
      "source": [
        "test_accuracy_sklearn = accuracy_score(y_test,y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on testing set: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}